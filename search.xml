<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>朴素贝叶斯模型</title>
      <link href="/2019/12/02/po-su-bei-xie-si/"/>
      <url>/2019/12/02/po-su-bei-xie-si/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="1朴素贝叶斯"><a href="#1朴素贝叶斯" class="headerlink" title="1朴素贝叶斯"></a>1朴素贝叶斯</h1><h2 id="1-1-朴素贝叶斯理论"><a href="#1-1-朴素贝叶斯理论" class="headerlink" title="1.1 朴素贝叶斯理论"></a>1.1 朴素贝叶斯理论</h2><p>朴素贝叶斯算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。朴素贝叶斯是贝叶斯决策理论的一部分，所以在讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。</p><h3 id="1-1-1-条件概率"><a href="#1-1-1-条件概率" class="headerlink" title="1.1.1 条件概率"></a>1.1.1 条件概率</h3><p>1．朴素贝叶斯法是典型的生成学习方法。生成方法由训练数据学习联合概率分布</p><p>$P(A,B)$，然后求得后验概率分布$P(B|A)$。具体来说，利用训练数据学习$P(A|B)$和$P(B)$的估计，得到联合概率分布：</p><script type="math/tex; mode=display">P(A,B)＝P(B)P(A|B)</script><p>概率估计方法可以是极大似然估计或贝叶斯估计。</p><p>2．朴素贝叶斯法的基本假设是条件独立性，</p><script type="math/tex; mode=display">\begin{aligned} P(A&=a | B=c_{k} )=P\left(A^{(1)}=a^{(1)}, \cdots, A^{(n)}=a^{(n)} | B=c_{k}\right) \\ &=\prod_{j=1}^{n} P\left(A^{(j)}=a^{(j)} | B=c_{k}\right) \end{aligned}</script><p>这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。</p><p>条件概率(Condittional probability)，就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示。</p><script type="math/tex; mode=display">P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><p>这就是条件概率计算公式。</p><h3 id="1-1-2-贝叶斯推断"><a href="#1-1-2-贝叶斯推断" class="headerlink" title="1.1.2 贝叶斯推断"></a>1.1.2 贝叶斯推断</h3><p>对条件概率公式进行变形，可以得到如下形式：</p><script type="math/tex; mode=display">P(A|B)=P(A)\frac{P(B|A)}{P(B)}</script><p>我们把P(A)称为”先验概率”（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。</p><p>P(A|B)称为”后验概率”（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。</p><p>P(B|A)/P(B)称为”可能性函数”（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。</p><p>所以，条件概率就是：</p><p>后验概率　＝　先验概率 ｘ 调整因子</p><p>这就是贝叶斯推断的含义。我们先预估一个”先验概率”，然后加入实验结果，看这个实验到底是增强还是削弱了”先验概率”，由此得到更接近事实的”后验概率”。</p><p>在这里，如果”可能性函数”P(B|A)/P(B)&gt;1，意味着”先验概率”被增强，事件A的发生的可能性变大；如果”可能性函数”=1，意味着B事件无助于判断事件</p><p>A的可能性；如果”可能性函数”&lt;1，意味着”先验概率”被削弱，事件A的可能性变小。</p><h3 id="1-1-3-全概率公式"><a href="#1-1-3-全概率公式" class="headerlink" title="1.1.3 全概率公式"></a>1.1.3 全概率公式</h3><script type="math/tex; mode=display">P(B)=P(B|A)P(A)+P(B|A')P(A')</script><p>这是全概率公式,它的意义是表示在事件A所有可能情况之下B发生的概率总和就是不考虑事件A时，事件B发生的概率，把这个全概率公式代入上面的条件概率公式再变形可以得到下面的写法</p><script type="math/tex; mode=display">p(A|B)=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A')P(A')}</script><p>这是条件概率的另一种写法</p><h3 id="1-1-4-朴素贝叶斯推断"><a href="#1-1-4-朴素贝叶斯推断" class="headerlink" title="1.1.4 朴素贝叶斯推断"></a>1.1.4 朴素贝叶斯推断</h3><p>贝叶斯和朴素贝叶斯的概念是不同的，区别就在于“朴素”二字，朴素贝叶斯对条件个概率分布做了条件独立性的假设。 比如下面的公式，假设有n个特征：</p><script type="math/tex; mode=display">p(a|X) = p(X|a)p(a)=p(x_1,x_2,x_3,\dots x_n|a)p(a)</script><p>由于每个特征都是独立的，我们可以进一步拆分公式</p><script type="math/tex; mode=display">p(a|X)=p(X|a)p(a)=\left[p(x_1|a)*p(x_2|a)*p(x_3|a)*\dots*p(x_n|a) \right]p(a)</script><h2 id="1-2-朴素贝叶斯优缺点："><a href="#1-2-朴素贝叶斯优缺点：" class="headerlink" title="1.2 朴素贝叶斯优缺点："></a>1.2 朴素贝叶斯优缺点：</h2><h3 id="1-2-1-朴素贝叶斯推断的一些优点："><a href="#1-2-1-朴素贝叶斯推断的一些优点：" class="headerlink" title="1.2.1 朴素贝叶斯推断的一些优点："></a>1.2.1 朴素贝叶斯推断的一些优点：</h3><ol><li>生成式模型，通过计算概率来进行分类，可以用来处理多分类问题。</li></ol><ol><li>对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。</li></ol><h3 id="1-2-2-朴素贝叶斯推断的一些缺点："><a href="#1-2-2-朴素贝叶斯推断的一些缺点：" class="headerlink" title="1.2.2 朴素贝叶斯推断的一些缺点："></a>1.2.2 朴素贝叶斯推断的一些缺点：</h3><ol><li>对输入数据的表达形式很敏感。</li></ol><ol><li>由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。</li></ol><ol><li>需要计算先验概率，分类决策存在错误率。</li></ol><h2 id="1-3朴素贝叶斯改进之拉普拉斯平滑"><a href="#1-3朴素贝叶斯改进之拉普拉斯平滑" class="headerlink" title="1.3朴素贝叶斯改进之拉普拉斯平滑"></a>1.3朴素贝叶斯改进之拉普拉斯平滑</h2><ol><li>利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。</li></ol><ol><li>如果新实例文本，包含这种概率为0的分词，那么最终的文本属于某个类别的概率也就是0了。显然，这样是不合理的，为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问题。</li></ol><ol><li>除此之外，另外一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。学过数学的人都知道，两个小数相乘，越乘越小，这样就造成了下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。</li></ol><h2 id="1-4总结"><a href="#1-4总结" class="headerlink" title="1.4总结"></a>1.4总结</h2><ol><li>在训练朴素贝叶斯分类器之前，要处理好训练集。</li></ol><ol><li>根据提取的分类特征将文本向量化，然后训练朴素贝叶斯分类器</li></ol><ol><li>去高频词汇数量的不同，对结果也是有影响的的。</li></ol><ol><li>拉普拉斯平滑对于改善朴素贝叶斯分类器的分类效果有着积极的作用。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降魔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型公式推导（下）</title>
      <link href="/2019/11/25/xian-xing-mo-xing-gong-shi-tui-dao-er/"/>
      <url>/2019/11/25/xian-xing-mo-xing-gong-shi-tui-dao-er/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="线性模型公式推导（下）"><a href="#线性模型公式推导（下）" class="headerlink" title="线性模型公式推导（下）"></a>线性模型公式推导（下）</h1><h2 id="1-1逻辑回归公式推导"><a href="#1-1逻辑回归公式推导" class="headerlink" title="1.1逻辑回归公式推导"></a>1.1逻辑回归公式推导</h2><h3 id="1-1-1逻辑回归推导思路"><a href="#1-1-1逻辑回归推导思路" class="headerlink" title="1.1.1逻辑回归推导思路"></a>1.1.1逻辑回归推导思路</h3><ol><li><p>逻辑回归是如何被建模出来的</p></li><li><p>逻辑回归的损失函数是什么以及是如何得出的</p></li><li><p>逻辑回归的参数如何得出</p></li></ol><p>逻辑回归又叫对数几率回归，那么它是怎么得出来的呢？这里介绍两种建模方法：</p><h3 id="1-1-2建模方法一"><a href="#1-1-2建模方法一" class="headerlink" title="1.1.2建模方法一"></a>1.1.2建模方法一</h3><h4 id="1-1-2-1广义线性模型"><a href="#1-1-2-1广义线性模型" class="headerlink" title="1.1.2.1广义线性模型"></a>1.1.2.1广义线性模型</h4><p>上一篇中的基础线性模型我们用$w^Tx+b$直接去拟合结果y。即：$y=w^Tx+b$</p><p>现在我们用线性模型去拟合结果y的函数g(y)（一般考虑单调可微函数）即：$g(y)=w^Tx+b$，这就是广义线性模型。</p><p>函数g()被称为联系函数。</p><h4 id="1-1-2-2逻辑回归公式的导出"><a href="#1-1-2-2逻辑回归公式的导出" class="headerlink" title="1.1.2.2逻辑回归公式的导出"></a>1.1.2.2逻辑回归公式的导出</h4><p>逻辑回归是在对一个二分类问题建模，所以它的样本有正与负两个分类。首先我们假设y为样本x为正例的概率。则(1-y)为x为反例的概率，他们两者的比值为$\frac{y}{1-y}$,我们对它取对数则可得$ln\frac{y}{1-y}$,然后我们用线性模型去拟合它则有:</p><script type="math/tex; mode=display">ln\frac{y}{1-y}=w^Tx+b</script><p>下面我们来推一下,<br>由上式可得： </p><script type="math/tex; mode=display">\frac{y}{1-y}=e^{w^Tx+b}</script><script type="math/tex; mode=display">y=e^{w^Tx+b}-e^{w^Tx+b}y</script><script type="math/tex; mode=display">(1+e^{w^Tx+b})y=e^{w^Tx+b}</script><script type="math/tex; mode=display">y=\frac{e^{w^Tx+b} } {(1+e^{w^Tx+b})}</script><script type="math/tex; mode=display">y=\frac{1}{ {e^{w^Tx+b}}^{-1}+1}</script><script type="math/tex; mode=display">y=\frac{1}{ {1+e^{-(w^Tx+b)} } }</script><p>通过上面的一系列变化我们就得出了逻辑回归的表达式了。</p><h3 id="1-1-3建模方法二"><a href="#1-1-3建模方法二" class="headerlink" title="1.1.3建模方法二"></a>1.1.3建模方法二</h3><h4 id="1-1-3-1指数族分布"><a href="#1-1-3-1指数族分布" class="headerlink" title="1.1.3.1指数族分布"></a>1.1.3.1指数族分布</h4><p>分布律满足下式即为指数族分布：</p><script type="math/tex; mode=display">p(y;\eta) = b(y) exp(\eta ^TT(y)-a(\eta))</script><p>逻辑回归是在对一个二分类问题建模，则我们可以很合理的假设该二分类符合伯努利分布</p><p>假设y取值为1时的概率为$\phi$</p><p>伯努力分布分布律为：</p><script type="math/tex; mode=display">\begin{aligned} p(y)&=\phi ^y(1-\phi)^{1-y} \\&=exp(ln(\phi^y(1-\phi)^{1-y}))\\&=exp(y ln\phi + (1-y)ln(1-\phi))\\&=exp(y(ln\phi-ln(1-\phi))+ln(1-\phi))\\&=exp\left(yln\left(\frac{\phi}{1-\phi}\right) +ln(1-\phi)\right)\end{aligned}</script><p>对照指数族分布的分布律可得：    $b(y)=1 \qquad \eta = ln\left(\frac{\phi}{1-\phi}\right) \qquad T(y) = y \qquad a(\eta) = -ln(1-\phi) = ln(1+e^{\eta})$</p><p>所以伯努力分布为指数族分布。</p><p>我们看一下由：</p><script type="math/tex; mode=display">\eta = ln\left(\frac{\phi}{1-\phi}\right)</script><p>可得：</p><script type="math/tex; mode=display">e^{\eta}=\frac{\phi}{1-\phi}</script><script type="math/tex; mode=display">e^{-\eta}=\frac{1}{\phi}-1</script><script type="math/tex; mode=display">\phi = \frac{1}{1+e^{-\eta}}</script><p>好，我们先推到这里下面我们看一下广义线性模型的三个性质。</p><h4 id="1-1-3-2根据广义线性模型三大假设建立逻辑回归模型表达式"><a href="#1-1-3-2根据广义线性模型三大假设建立逻辑回归模型表达式" class="headerlink" title="1.1.3.2根据广义线性模型三大假设建立逻辑回归模型表达式"></a>1.1.3.2根据广义线性模型三大假设建立逻辑回归模型表达式</h4><p>通过下面三条假设构建出来的模型就是广义线性模型：</p><ol><li>在给定x的条件下，假设y服从某个指数族分布。</li><li>在给定x的条件下，我们的目标是得到一个模型h(x)能预测出T(y)的期望值。</li><li>假设该指数族分布中的参数$\eta与x呈线性关系：\eta = w^Tx$</li></ol><p>下面我们就根据这三条假设来建模：</p><p>我们前面在介绍指数分布族的时候已经证明了伯努利分布是属于指数分布族的了，而逻辑回归也是建立在伯努利分布的基础上所以第一条假设的已经满足。<br>下面看第二条假设，由上文中T(y)]=y 可得：</p><script type="math/tex; mode=display">h(x)=E[T(y)]=E(y)</script><script type="math/tex; mode=display">E[y]=1\cdot p(y=1)+0\cdot p(y=0)=p(y=1)=\phi</script><script type="math/tex; mode=display">h(x)=\phi</script><p>由上面的式（1.0可得）</p><script type="math/tex; mode=display">h(x)= \frac{1}{1+e^{-\eta}}</script><p>再根据第三条假设可得：</p><script type="math/tex; mode=display">h(x)= \frac{1}{1+e^{-w^Tx}}</script><p>这样我们便得出了逻辑回归的模型表达式了</p><h3 id="1-1-4用极大似然估计法构建逻辑回归的损失函数"><a href="#1-1-4用极大似然估计法构建逻辑回归的损失函数" class="headerlink" title="1.1.4用极大似然估计法构建逻辑回归的损失函数"></a>1.1.4用极大似然估计法构建逻辑回归的损失函数</h3><p>$y_1,y_2,\cdots,y_m是独立同分布的，所以他们的联合概率密度函数为L(w)=\prod\limits_{i=1}^mp(y_i|x_i;w,b)\qquad此为似然函数$</p><p>lnL(w)与L(w)有相同最大值点 $lnL(w)=\sum\limits_{i=1}^mlnp(y_i|x_i;w,b)$</p><script type="math/tex; mode=display">p(y|x)=\begin{cases}\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}& \text{y=1}\\\frac1{1+e^{w^Tx+b}}& \text{y=0}\end{cases}</script><p>即：<br>$p(y=1|x) = \frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\qquad$<br>$p(y=0|x) = \frac1{1+e^{w^Tx+b}}$</p><p>令$w^Tx+b = \hat w \hat x$</p><p>$p(y=1|x) = \frac{e^{\hat w \hat x} }{1+e^{\hat w \hat x} }=h_{\hat w}(\hat  x)$</p><p>$p(y=0|x) = \frac{1}{1+e^{\hat w \hat x}}=1-h_{\hat w}(\hat x)$</p><p>把上面的两个式子写在一起为：</p><script type="math/tex; mode=display">p(y|\hat {x})=h_{\hat w}(\hat x)^y( 1-h_{\hat w}(\hat x))^{1-y}\qquad\qquad</script><p>似然函数为：</p><script type="math/tex; mode=display">L(\hat w)=\prod_{i=1}^m h_{\hat w}(\hat x)^y( 1-h_{\hat w}(\hat x))^{1-y}</script><p>对数似然函数为：</p><script type="math/tex; mode=display">\begin{aligned} \log(L(\hat w))&=\prod_{i=1}^m h_{\hat w}(\hat x)^y_i( 1-h_{\hat w}(\hat x))^{(1-y_i)}\\&=\sum_{i=1}^m [y_ilog(h_{\hat w}(\hat x)_i)+(1-y_i)log(1-h_{\hat w}(\hat x)_i)]\\\end{aligned}</script><p>根据上面的似然函数构造如下一个损失函数，极大化上面的对数似然函数，等于最小化下面的损失函数：</p><script type="math/tex; mode=display">J(\hat w)=-\frac{1}{m}\sum_{i=1}^m [y_ilog(h_{\hat w}(\hat x)_i)+(1-y_i)log(1-h_{\hat w}(\hat x)_i)]\qquad\qquad(1)</script><h3 id="1-1-5用梯度下降法求解损失函数的最优参数"><a href="#1-1-5用梯度下降法求解损失函数的最优参数" class="headerlink" title="1.1.5用梯度下降法求解损失函数的最优参数"></a>1.1.5用梯度下降法求解损失函数的最优参数</h3><p>$令\hat w = w \qquad\frac{e^{\hat w \hat x} }{1+e^{\hat w \hat x} }=h_{\hat w}(\hat x_i)=\phi(z^{(i)})$</p><p>则式（1）可以改写为：</p><script type="math/tex; mode=display">J(w)= -\frac{1}{m}\sum_{i=1}^m [y_ilog(\phi(z^{(i)})+(1-y_i)log(1-\phi(z^{(i)})]</script><p>梯度下降法每次迭代，新的$w为:  w-\alpha \cdot \frac{\partial J( w)}{\partial  w}\quad \alpha 为学习率$</p><p>sigmoid函数的导数：</p><p>$\phi’(z)=\phi(z)(1-\phi(z))$</p><p>$w是所有参数组成的向量，w_j为w的第j个分量$</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial J(w)}{w_{j}} &=-\sum_{i=1}^{m}\left(y^{(i)} \frac{1}{\phi\left(z^{(i)}\right)}-\left(1-y^{(i)}\right) \frac{1}{1-\phi\left(z^{(i)}\right)}\right) \frac{\partial \phi\left(z^{(i)}\right)}{\partial w_{j}} \\ &=-\sum_{i=1}^{m}\left(y^{(i)} \frac{1}{\phi\left(z^{(i)}\right)}-\left(1-y^{(i)}\right) \frac{1}{1-\phi\left(z^{(i)}\right)}\right) \phi\left(z^{(i)}\right)\left(1-\phi\left(z^{(i)}\right)\right) \frac{\partial z^{(i)}}{\partial w_{j}} \\ &=-\sum_{i=1}^{m}\left(y^{(i)}\left(1-\phi\left(z^{(i)}\right)\right)-\left(1-y^{(i)}\right) \phi\left(z^{(i)}\right)\right) x_{j}^{(i)} \\ &=-\sum_{i=1}^{m}\left(y^{(i)}-\phi\left(z^{(i)}\right)\right) x_{j}^{(i)} \end{aligned}</script><p>$所以w_j更新函数为: w_j+\alpha\sum \limits_{i=1}^{m}\left(  y^{(i)}-\phi\left(z^{(i)}\right)\right) x_{j}^{(i)}$</p><p>利用求得的更新函数，迭代多次后便可求出一个可以接受的参数值了。</p><h2 id="1-2感知机模型公式推导"><a href="#1-2感知机模型公式推导" class="headerlink" title="1.2感知机模型公式推导"></a>1.2感知机模型公式推导</h2><h3 id="1-2-1感知机模型函数"><a href="#1-2-1感知机模型函数" class="headerlink" title="1.2.1感知机模型函数"></a>1.2.1感知机模型函数</h3><script type="math/tex; mode=display">f(x)=sign(w\cdot x+b)</script><p>sign(a)函数表示若a&gt;0则sign(a)=1; 若a&lt;0则sign(a)=-1。</p><h3 id="1-2-2感知机的损失函数"><a href="#1-2-2感知机的损失函数" class="headerlink" title="1.2.2感知机的损失函数"></a>1.2.2感知机的损失函数</h3><p>感知机的损失函数是由误分类点到决策边界的距离所定义的。$假设x_i为分类错误的点，它到分类超平面w\cdot x+b=0的距离为:$</p><script type="math/tex; mode=display">\frac{|w\cdot x_i+b|}{\parallel w\parallel}=\frac{-y_i(w\cdot x_i+b)}{\parallel w\parallel}</script><p>$其中\parallel w\parallel=\sqrt{w_1^2+w_2^2+\cdots +w_n^2}$</p><p>若所有误分类的点所属的集合为M：则所有误分类点到超平面的总距离为：</p><script type="math/tex; mode=display">\sum_{x_i \in M}\frac{-y_i(w\cdot x_i+b)}{\parallel w\parallel}</script><p>因为${\parallel w\parallel}$为常数，所以上面的式子等价于下面的$L(w,b)$</p><script type="math/tex; mode=display">L(w,b)=-\sum_{x_i \in M} y_i(w \cdot x_i+b)</script><p>$L(w,b)$则为感知机的损失函数</p><h3 id="1-2-3最小化损失函数"><a href="#1-2-3最小化损失函数" class="headerlink" title="1.2.3最小化损失函数"></a>1.2.3最小化损失函数</h3><p>$对于单个样本(x_i,y_i)$</p><p>$L’_w(w,b)= \frac{\partial y_i(w \cdot x_i+b)}{\partial w}=-y_ix_i$</p><p>$L’_b(w,b)= \frac{\partial y_i(w \cdot x_i+b)}{\partial b}=-y_i$</p><p>接下来就是最小化损失函数从而找出参数的最优解</p><h3 id="1-2-4用随机梯度下降法找到w和b最优解"><a href="#1-2-4用随机梯度下降法找到w和b最优解" class="headerlink" title="1.2.4用随机梯度下降法找到w和b最优解"></a>1.2.4用随机梯度下降法找到w和b最优解</h3><p>首先选初始值$(w_0,b_0)$</p><p>在训练集中选取数据$(x_i,y_i)$</p><p>如果它是错误分类的点，即：$y_i(w\cdot x_i+b)\le0$</p><p>$则 w 更新为 w+\eta y_ix_i \qquad (\eta 为学习率)$</p><p>$b更新为 b+\eta y_i$</p><p>重复这个过程直到所有误分类的点都被正确分类为止。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降魔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型代码实现</title>
      <link href="/2019/11/25/xian-xing-mo-xing-dai-ma-shi-xian/"/>
      <url>/2019/11/25/xian-xing-mo-xing-dai-ma-shi-xian/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="线性模型实现"><a href="#线性模型实现" class="headerlink" title="线性模型实现"></a>线性模型实现</h1><h2 id="创建一个通用的Estimator"><a href="#创建一个通用的Estimator" class="headerlink" title="创建一个通用的Estimator"></a>创建一个通用的Estimator</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">import warningswarnings.filterwarnings('ignore')import loggingimport autograd.numpy as npfrom autograd import gradnp.random.seed(1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">class BaseEstimator(object):    X = None    y = None    y_required = True    fit_required = True    def _setup_input(self, X, y=None):        """        参数        ----------        X : 实例        y : 预测结果        """        if not isinstance(X, np.ndarray):            X = np.array(X)        if X.size == 0:            raise ValueError("Number of features must be > 0")        if X.ndim == 1:            self.n_samples, self.n_features = 1, X.shape        else:            self.n_samples, self.n_features = X.shape[0], np.prod(X.shape[1:])        self.X = X参数        if self.y_required:            if y is None:                raise ValueError("Missed required argument y")            if not isinstance(y, np.ndarray):                y = np.array(y)            if y.size == 0:                raise ValueError("Number of targets must be > 0")        self.y = y    def fit(self, X, y=None):        self._setup_input(X, y)    def predict(self, X=None):        if not isinstance(X, np.ndarray):            X = np.array(X)        if self.X is not None or not self.fit_required:            return self._predict(X)        else:            raise ValueError("You must call `fit` before `predict`")    def _predict(self, X=None):        raise NotImplementedError()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">EPS = 1e-15<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="定义一些评价指标"><a href="#定义一些评价指标" class="headerlink" title="定义一些评价指标"></a>定义一些评价指标</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">def accuracy(actual, predicted):    return 1.0 - classification_error(actual, predicted)def mean_squared_error(actual, predicted):    return np.mean(squared_error(actual, predicted))def binary_crossentropy(actual, predicted):    predicted = np.clip(predicted, EPS, 1 - EPS)    return np.mean(-np.sum(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted)))def squared_error(actual, predicted):    return (actual - predicted) ** 2def classification_error(actual, predicted):    return (actual != predicted).sum() / float(actual.shape[0])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="基于梯度下降法的基本回归模型"><a href="#基于梯度下降法的基本回归模型" class="headerlink" title="基于梯度下降法的基本回归模型"></a>基于梯度下降法的基本回归模型</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">class BasicRegression(BaseEstimator):    def __init__(self, lr=0.001, penalty="None", C=0.01, tolerance=0.0001, max_iters=1000):        """基于梯度下降法的基本回归模型        参数        ----------        lr : 学习率        penalty : 正则化项        C : 正则化系数        tolerance : 梯度下降更新停止阈值        max_iters : 最大迭代次数        """        self.C = C        self.penalty = penalty        self.tolerance = tolerance        self.lr = lr        self.max_iters = max_iters        self.errors = []        self.theta = []        self.n_samples, self.n_features = None, None        self.cost_func = None    def _loss(self, w):        raise NotImplementedError()    def init_cost(self):        raise NotImplementedError()    def _add_penalty(self, loss, w):        """Apply regularization to the loss."""        if self.penalty == "l1":            loss += self.C * np.abs(w[1:]).sum()        elif self.penalty == "l2":            loss += (0.5 * self.C) * (w[1:] ** 2).sum()        return loss    def _cost(self, X, y, theta):        prediction = X.dot(theta)        error = self.cost_func(y, prediction)        return error    def fit(self, X, y=None):        self._setup_input(X, y)        self.init_cost()        self.n_samples, self.n_features = X.shape        # Initialize weights + bias term        self.theta = np.random.normal(size=(self.n_features + 1), scale=0.5)        # Add an intercept column        self.X = self._add_intercept(self.X)        self._train()    @staticmethod    def _add_intercept(X):        b = np.ones([X.shape[0], 1])        return np.concatenate([b, X], axis=1)    def _train(self):        self.theta, self.errors = self._gradient_descent()        logging.info(" Theta: %s" % self.theta.flatten())    def _predict(self, X=None):        X = self._add_intercept(X)        return X.dot(self.theta)    def _gradient_descent(self):        theta = self.theta        errors = [self._cost(self.X, self.y, theta)]        # Get derivative of the loss function        cost_d = grad(self._loss)        for i in range(1, self.max_iters + 1):            # Calculate gradient and update theta            delta = cost_d(theta)            theta -= self.lr * delta            errors.append(self._cost(self.X, self.y, theta))            logging.info("Iteration %s, error %s" % (i, errors[i]))            error_diff = np.linalg.norm(errors[i - 1] - errors[i])            if error_diff < self.tolerance:                logging.info("Convergence has reached.")                break        return theta, errors<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="定义基于梯度下降法的线性回归与逻辑回归模型"><a href="#定义基于梯度下降法的线性回归与逻辑回归模型" class="headerlink" title="定义基于梯度下降法的线性回归与逻辑回归模型"></a>定义基于梯度下降法的线性回归与逻辑回归模型</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">class LinearRegression(BasicRegression):    """Linear regression with gradient descent optimizer."""    def _loss(self, w):        loss = self.cost_func(self.y, np.dot(self.X, w))        return self._add_penalty(loss, w)    def init_cost(self):        self.cost_func = mean_squared_errorclass LogisticRegression(BasicRegression):    """Binary logistic regression with gradient descent optimizer."""    def init_cost(self):        self.cost_func = binary_crossentropy    def _loss(self, w):        loss = self.cost_func(self.y, self.sigmoid(np.dot(self.X, w)))        return self._add_penalty(loss, w)    @staticmethod    def sigmoid(x):        return 0.5 * (np.tanh(0.5 * x) + 1)    def _predict(self, X=None):        X = self._add_intercept(X)        return self.sigmoid(X.dot(self.theta))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="用一个小例子来使用上面实现的线性回归与逻辑回归模型"><a href="#用一个小例子来使用上面实现的线性回归与逻辑回归模型" class="headerlink" title="用一个小例子来使用上面实现的线性回归与逻辑回归模型"></a>用一个小例子来使用上面实现的线性回归与逻辑回归模型</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">import loggingtry:    from sklearn.model_selection import train_test_splitexcept ImportError:    from sklearn.cross_validation import train_test_splitfrom sklearn.datasets import make_classificationfrom sklearn.datasets import make_regression# Change to DEBUG to see convergencelogging.basicConfig(level=logging.ERROR)def regression():    # Generate a random regression problem    X, y = make_regression(        n_samples=10000, n_features=100, n_informative=75, n_targets=1, noise=0.05, random_state=1111, bias=0.5    )    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1111)    model = LinearRegression(lr=0.01, max_iters=2000, penalty="l2", C=0.03)    model.fit(X_train, y_train)    predictions = model.predict(X_test)    print("regression mse", mean_squared_error(y_test, predictions))def classification():    # Generate a random binary classification problem.    X, y = make_classification(        n_samples=1000, n_features=100, n_informative=75, random_state=1111, n_classes=2, class_sep=2.5    )    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)    model = LogisticRegression(lr=0.01, max_iters=500, penalty="l1", C=0.01)    model.fit(X_train, y_train)    predictions = model.predict(X_test)    print("classification accuracy", accuracy(y_test, predictions))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">regression()classification()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>regression mse 67.1561814262031classification accuracy 0.97</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降魔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型公式推导（上）</title>
      <link href="/2019/11/25/xian-xing-mo-xing-gong-shi-tui-dao-yi/"/>
      <url>/2019/11/25/xian-xing-mo-xing-gong-shi-tui-dao-yi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="1线性模型公式推导（上）"><a href="#1线性模型公式推导（上）" class="headerlink" title="1线性模型公式推导（上）"></a>1线性模型公式推导（上）</h1><p>本篇对线性模型中的一元线性回归与多元线性回归模型进行了推导</p><h2 id="1-1一元线性回归公式推导"><a href="#1-1一元线性回归公式推导" class="headerlink" title="1.1一元线性回归公式推导"></a>1.1一元线性回归公式推导</h2><h3 id="1-1-1推导流程及思路"><a href="#1-1-1推导流程及思路" class="headerlink" title="1.1.1推导流程及思路"></a>1.1.1推导流程及思路</h3><ol><li><p>先由最小二乘法导出损失函数$E(w,b)$</p></li><li><p>证明损失函数$E(w,b)$是关于w和b的凸函数。（因为只有凸函数才有最小值）</p></li><li><p>对损失函数$E(w,b)$关于w 和 b分别求一阶导数</p></li><li><p>令一阶偏导数等于0解出w和b。</p></li></ol><h3 id="1-1-2核心思想是最小二乘法"><a href="#1-1-2核心思想是最小二乘法" class="headerlink" title="1.1.2核心思想是最小二乘法"></a>1.1.2核心思想是最小二乘法</h3><p>一元线性回归公式：$f(x_i)=wx_i+b$</p><h4 id="1-1-2-1由最小二乘法导出平方损失-E-w-b"><a href="#1-1-2-1由最小二乘法导出平方损失-E-w-b" class="headerlink" title="1.1.2.1由最小二乘法导出平方损失 E(w,b)"></a>1.1.2.1由最小二乘法导出平方损失 E(w,b)</h4><script type="math/tex; mode=display">\begin{aligned} E(w,b)    &= \sum _{i=1}^m(y_i-f(x_i))^2\\    &=\sum_{i=1}^m(y_i-(wx_i+b))^2\\    &=\sum_{i=1}^m(y_i-wx_i-b)^2 \qquad\qquad\qquad(1.1)\end{aligned}</script><h4 id="1-1-2-2我们的目的是求出损失函数E-w-b-取最小值时，w与b的解"><a href="#1-1-2-2我们的目的是求出损失函数E-w-b-取最小值时，w与b的解" class="headerlink" title="1.1.2.2我们的目的是求出损失函数E(w,b)取最小值时，w与b的解"></a>1.1.2.2我们的目的是求出损失函数E(w,b)取最小值时，w与b的解</h4><script type="math/tex; mode=display">(w^*,b^*)= \underset{(w,b)}{\operatorname{argmin}}\sum_{i=1}^m(y_i-wx_i-b)^2</script><h3 id="1-1-3判断损失函数E-w-b-的凹凸性"><a href="#1-1-3判断损失函数E-w-b-的凹凸性" class="headerlink" title="1.1.3判断损失函数E(w,b)的凹凸性"></a>1.1.3判断损失函数E(w,b)的凹凸性</h3><p>由凹凸函数的判定定理：设E(w,b)在区域D上具有二阶连续偏导数</p><p>记$A=E’’_{ww}(w,b),  B=E’’_{wb}(w,b), C=E’’_{bb}(w,b)$ 则：$A&gt;0, 且 AC-B^2\geq0$ 时，E(w,b)在区域D上是凸函数；</p><h4 id="1-1-3-1根据上述定理我们先来求A，A为E-w-b-对w的二阶偏导数-E’’-ww-w-b"><a href="#1-1-3-1根据上述定理我们先来求A，A为E-w-b-对w的二阶偏导数-E’’-ww-w-b" class="headerlink" title="1.1.3.1根据上述定理我们先来求A，A为E(w,b)对w的二阶偏导数$E’’_{ww}(w,b)$"></a>1.1.3.1根据上述定理我们先来求A，A为E(w,b)对w的二阶偏导数$E’’_{ww}(w,b)$</h4><ol><li>我们先求E(w,b)对w的一阶偏导数：</li></ol><script type="math/tex; mode=display">\begin{aligned} E'_w(w,b)    &=\frac{\partial E(w,b)}{\partial w}\\    &=\frac{\partial }{\partial w}\bigg[\sum_{i=1}^m(y_i-wx_i-b)^2\bigg]\\    &=\sum_{i=1}^m\frac{\partial }{\partial w}(y_i-wx_i-b)^2\\    &=\sum_{i=1}^m2\cdot(y_i-wx_i-b)\cdot(-x_i)\\    &=2\bigg(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)\qquad\qquad\qquad(1.2)\end{aligned}</script><ol><li>然后根据上述结果进而求出A:</li></ol><script type="math/tex; mode=display">\begin{aligned}A    &=E''_{ww}(w,b)\\    &=\frac{\partial^2 E(w,b)}{\partial w^2}\\    &= \frac{\partial }{\partial w}\bigg(\frac{\partial E(w,b)}{\partial w}\bigg)\\    &= \frac{\partial }{\partial w}\bigg[2\bigg(w\sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)\bigg]\\    &=\frac{\partial }{\partial w}\bigg[2w\sum_{i=1}^m x_i^2\bigg]\\    &=2\sum_{i=1}^mx_i^2 \qquad\qquad\qquad       这就是A=E''_{ww}(w,b)\end{aligned}</script><h4 id="1-1-3-2接下来我们来求B，B为E-w-b-对w和b的二阶混合偏导数-E’’-wb-w-b"><a href="#1-1-3-2接下来我们来求B，B为E-w-b-对w和b的二阶混合偏导数-E’’-wb-w-b" class="headerlink" title="1.1.3.2接下来我们来求B，B为E(w,b)对w和b的二阶混合偏导数: $E’’_{wb}(w,b)$"></a>1.1.3.2接下来我们来求B，B为E(w,b)对w和b的二阶混合偏导数: $E’’_{wb}(w,b)$</h4><script type="math/tex; mode=display">\begin{aligned} E''_{wb}(w,b)    &=\frac{\partial^2 E(w,b)}{\partial w \partial b}\\    &=\frac{\partial}{\partial b}\bigg(\frac{\partial E(w,b)}{\partial w}\bigg)\\    &=\frac{\partial}{\partial b}\bigg[2\bigg(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)\bigg]\\    &=\frac{\partial}{\partial b}\bigg[-2\sum_{i=1}^m(y_i-b)x_i\bigg]\\    &=\frac{\partial}{\partial b}\bigg(-2\sum_{i=1}^my_ix_i+2\sum_{i=1}^mbx_i\bigg)\\    &=2\sum_{i=1}^mx_i \qquad\qquad\qquad       这就是B=E''_{wb}(w,b)\end{aligned}</script><h4 id="1-1-3-3最后我们来求C，C为E-w-b-对b的二阶偏导数-E’’-bb-w-b"><a href="#1-1-3-3最后我们来求C，C为E-w-b-对b的二阶偏导数-E’’-bb-w-b" class="headerlink" title="1.1.3.3最后我们来求C，C为E(w,b)对b的二阶偏导数$E’’_{bb}(w,b)$"></a>1.1.3.3最后我们来求C，C为E(w,b)对b的二阶偏导数$E’’_{bb}(w,b)$</h4><ol><li>我们先求E(w,b)对b的一阶偏导数：$E’_b(w,b)=\frac{\partial E(w,b)}{\partial b}$</li></ol><script type="math/tex; mode=display">\begin{aligned} \frac{\partial E(w,b)}{\partial b}    &=\frac{\partial}{\partial b}\bigg[ \sum_{i=1}^m(y_i-wx_i-b)^2\bigg]\\    &=\sum_{i=1}^m\frac{\partial}{\partial b}(y_i-wx_i-b)^2\\    &=\sum_{i=1}^m2\cdot(y_i-wx_i-b)\cdot(-1)\\    &=2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)\qquad\qquad\qquad(1.3)\end{aligned}</script><ol><li>然后根据上述结果进而求出C。 $C=E’’_{bb}(w,b)=\frac{\partial^2 E(w,b)}{\partial b^2}$</li></ol><script type="math/tex; mode=display">\begin{aligned} \frac{\partial^2 E(w,b)}{\partial b^2}    &=\frac{\partial}{\partial b}\bigg(\frac{\partial E(w,b)}{\partial b}\bigg)\\    &=\frac{\partial}{\partial b}\bigg[ 2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)\bigg]\\    &=2m\qquad\qquad\qquad       这就是C=E''_{bb}(w,b)\end{aligned}</script><h4 id="1-1-3-4下面我们根据上面算出的ABC的值，来证明E-w-b-的凹凸性"><a href="#1-1-3-4下面我们根据上面算出的ABC的值，来证明E-w-b-的凹凸性" class="headerlink" title="1.1.3.4下面我们根据上面算出的ABC的值，来证明E(w,b)的凹凸性"></a>1.1.3.4下面我们根据上面算出的ABC的值，来证明E(w,b)的凹凸性</h4><p>我把定理贴过来再看一眼：设E(w,b)在区域D上具有二阶连续偏导数</p><p>记$A=E’’_{ww}(w,b),  B=E’’_{wb}(w,b), C=E’’_{bb}(w,b)$ 则：$A&gt;0, 且 AC-B^2\geq0$ 时，E(w,b)在区域D上是凸函数；</p><p>由上文可得：$A=2\sum\limits_{i=1}^mx_i^2\qquad\qquad B=2\sum\limits_{i=1}^mx_i\qquad\qquad C=2m$</p><p>A显然是大于0的。</p><script type="math/tex; mode=display">\begin{aligned} AC-B^2    &=2m\cdot2\sum\limits_{i=1}^mx_i^2-\bigg( 2\sum\limits_{i=1}^mx_i\bigg)^2\\    &=4m\sum\limits_{i=1}^mx_i^2-4m\cdot\frac1m\cdot\bigg( \sum\limits_{i=1}^mx_i\bigg)^2\\    &=4m\sum\limits_{i=1}^mx_i^2-4m\cdot\bar x\cdot \sum\limits_{i=1}^mx_i\\    &=4m\bigg( \sum\limits_{i=1}^mx_i^2-\sum\limits_{i=1}^mx_i\bar x\bigg)\\    &=4m\sum\limits_{i=1}^m(x_i^2-x_i\bar x) \\    &=4m\sum\limits_{i=1}^m(x_i^2-x_i\bar x-x_i\bar x+x_i\bar x)\qquad\longleftarrow\qquad因为\sum\limits_{i=1}^mx_i\bar x=m\bar x\bar x=\sum\limits_{i=1}^m\bar x^2\\    &=4m\sum\limits_{i=1}^m(x_i-\bar x)^2 \geq0\end{aligned}</script><p>由上可以证明：$\color{red}{E(w,b)是关于w和b的凸函数.}$</p><h3 id="1-1-4算出损失函数取最小值时w与b的值"><a href="#1-1-4算出损失函数取最小值时w与b的值" class="headerlink" title="1.1.4算出损失函数取最小值时w与b的值"></a>1.1.4算出损失函数取最小值时w与b的值</h3><p>根据定理：设$E(w,b)$是在D内有连续偏导数的凸函数</p><script type="math/tex; mode=display">(w^*，b^*)\in D\quad 且:E'_w(w{^*} , b {^*} )=0,E'_b(w^*,b^*)=0 \quad则:E(w^*,b^*)必为E(w,b)在D内的最小值。</script><p>$前面的小节我们证明了损失函数E(w,b)是凸函数，所以E(w,b)取最小值时，w与b的解满足E(w,b)的一阶偏导数为0$</p><h4 id="1-1-4-1先来求b"><a href="#1-1-4-1先来求b" class="headerlink" title="1.1.4.1先来求b"></a>1.1.4.1先来求b</h4><p>令 $\frac{\partial E(w,b)}{\partial b}=0$  (即：式1.3=0)</p><script type="math/tex; mode=display">\frac{\partial E(w,b)}{\partial b}=2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)=0</script><script type="math/tex; mode=display">mb = \sum_{i=1}^m(y_i-wx_i)</script><script type="math/tex; mode=display">\begin{aligned}b^*    &=\frac1m\sum_{i=1}^m(y_i-wx_i)\\    &=\frac1m\sum_{i=1}^my_i-w\cdot\frac1m\sum_{i=1}^mx_i\\    &=\bar y-w\bar x\\\end{aligned}</script><h4 id="1-1-4-2再来求w"><a href="#1-1-4-2再来求w" class="headerlink" title="1.1.4.2再来求w"></a>1.1.4.2再来求w</h4><p>$令\frac{\partial E(w,b)}{\partial w}=0$  (即：式1.2=0)</p><script type="math/tex; mode=display">\frac{\partial E(w,b)}{\partial w}=2\bigg(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)=0</script><script type="math/tex; mode=display">w\sum_{i=1}^mx_i^2=\sum_{i=1}^my_ix_i-\sum_{i=1}^mbx_i</script><script type="math/tex; mode=display">w\sum_{i=1}^mx_i^2=\sum_{i=1}^my_ix_i-\sum_{i=1}^m(\bar y-w\bar x)x_i</script><script type="math/tex; mode=display">w\sum_{i=1}^mx_i^2=\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i+w\bar x\sum_{i=1}^mx_i</script><script type="math/tex; mode=display">w\bigg(\sum_{i=1}^mx_i^2-\bar x\sum_{i=1}^mx_i\bigg)=\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i</script><script type="math/tex; mode=display">w^*=\frac{\sum\limits_{i=1}^my_ix_i-\bar y\sum\limits_{i=1}^mx_i}{\sum\limits_{i=1}^mx_i^2-\bar x\sum\limits_{i=1}^mx_i}\qquad\qquad(1.4)</script><p>因为：</p><script type="math/tex; mode=display">\bar y\sum\limits_{i=1}^mx_i=\frac1m\sum\limits_{i=1}^my_i\sum\limits_{i=1}^mx_i=\bar x\sum\limits_{i=1}^my_i\qquad\qquad(1.5)</script><script type="math/tex; mode=display">\bar x\sum\limits_{i=1}^mx_i=\frac1m\sum\limits_{i=1}^mx_i\sum\limits_{i=1}^mx_i=\frac1m(\sum\limits_{i=1}^mx_i)^2\qquad\qquad(1.6)</script><p>将1.5和1.6代入1.4可得：</p><script type="math/tex; mode=display">w^*=\frac{\sum\limits_{i=1}^my_ix_i-\bar x\sum\limits_{i=1}^my_i}{\sum\limits_{i=1}^mx_i^2-\frac1m(\sum\limits_{i=1}^mx_i)^2}=\frac{\sum\limits_{i=1}^my_i(x_i-\bar x)}{\sum\limits_{i=1}^mx_i^2-\frac1m(\sum\limits_{i=1}^mx_i)^2}</script><p>到此为止我们就求出了损失函数取最小值时的w和b的值。</p><h2 id="1-2多元线性回归公式推导"><a href="#1-2多元线性回归公式推导" class="headerlink" title="1.2多元线性回归公式推导"></a>1.2多元线性回归公式推导</h2><h3 id="1-2-1先对公式进行一个简单的变形"><a href="#1-2-1先对公式进行一个简单的变形" class="headerlink" title="1.2.1先对公式进行一个简单的变形"></a>1.2.1先对公式进行一个简单的变形</h3><p>多元线性回归公式：（大家一定要记住：多元线性回归里变量为向量）</p><script type="math/tex; mode=display">\begin{aligned}f(x_i)&=w^Tx_i+b\\&=(w_1 \   w_2\  \cdots \   w_d)  \left(\begin{matrix}x_{i1}\\ x_{i2}  \\ \vdots \\ x_{id}\end{matrix}\right) +b\\&=w_1x_{i1}+w_2x_{i2}+\dots+w_dx_{id}+b\end{aligned}</script><p>现在我们令：</p><p>$b=w_{d+1}  $</p><script type="math/tex; mode=display">\begin{aligned}f(x_i)&=(w_1 \   w_2\  \cdots \   w_d \   )  \left(\begin{matrix}x_{i1}\\ x_{i2}  \\ \vdots \\ x_{id}\end{matrix}\right) +b\\&=(w_1 \   w_2\  \cdots \   w_d \    w_{d+1})  \left(\begin{matrix}x_{i1}\\ x_{i2}  \\ \vdots \\ x_{id} \\ 1\end{matrix}\right)=\hat w \hat x_i\end{aligned}</script><p>通过上面的方法，我们把w和b组合成了$\hat w$ 则：</p><script type="math/tex; mode=display">f(\hat x_i)=\hat w^T\hat x_i</script><h3 id="1-2-2推导流程及思路"><a href="#1-2-2推导流程及思路" class="headerlink" title="1.2.2推导流程及思路"></a>1.2.2推导流程及思路</h3><ol><li>由最小二乘法导出损失函数$E_{\hat w}$。</li></ol><ol><li>证明损失函数$E_{\hat w}是关于\hat w$的凸函数。</li></ol><ol><li>对损失函数$E_{\hat w}关于\hat w$求一阶导数。</li></ol><ol><li>令一阶导数等于0解出$\hat w$</li></ol><h3 id="1-2-3核心思想依然是最小二乘法"><a href="#1-2-3核心思想依然是最小二乘法" class="headerlink" title="1.2.3核心思想依然是最小二乘法"></a>1.2.3核心思想依然是最小二乘法</h3><h4 id="1-2-3-1由最小二乘法导出损失函数"><a href="#1-2-3-1由最小二乘法导出损失函数" class="headerlink" title="1.2.3.1由最小二乘法导出损失函数"></a>1.2.3.1由最小二乘法导出损失函数</h4><p>损失函数(均方误差)为：$E_{\hat{w}} = \sum\limits_{i=1}^m(y_i-f(\hat{x_i}))^2=\sum\limits_{i=1}^m(y_i-\hat{w} ^T  \hat{x_i})^2$</p><p>$X=\left(\begin{matrix}x_{11} &amp; x_{12} &amp;\cdots&amp;x_{1d}&amp;1\\ x_{12} &amp;x_{22} &amp;  \cdots&amp;  x_{2d}&amp;1 \\ \vdots &amp; \vdots  &amp;&amp;\vdots&amp;\vdots\\ x_{m1}&amp;x_{m2}&amp;\cdots&amp;x_{md}&amp;1\end{matrix}\right)= \left(\begin{matrix}x_1^T&amp;1\\ x_2^T&amp;1 \\ \vdots &amp;\vdots\\ x_m^T&amp;1\end{matrix}\right)=\left(\begin{matrix}\hat x_1^T\\ \hat x_2^T \\ \vdots \\ \hat x_m^T\end{matrix}\right)$</p><p>$Y=\left(\begin{matrix}y_1\\y_2 \\ \vdots \\y_m\end{matrix}\right)$</p><script type="math/tex; mode=display">\begin{aligned}E_{\hat{w}} &=\sum\limits_{i=1}^m\left(y_i-\hat w^T \hat x_i\right)^2\\&=\left( y_1-\hat w^T\hat x_1\right)^2+\left( y_2-\hat w^T\hat x_2\right)^2+\cdots+\left( y_m-\hat w^T\hat x_m\right)^2\\&=\left( y_1-\hat w^T\hat x_1 \quad  y_2-\hat w^T\hat x_2  \quad  \cdots \quad   y_m-\hat w^T\hat x_m\right)     \left(\begin{matrix} y_1-\hat w^T\hat x_1\\y_2-\hat w^T\hat x_2 \\ \vdots \\y_m-\hat w^T\hat x_m\end{matrix}\right)\\\end{aligned}</script><p>因为： $\hat w^T\hat x_i = \hat x_i^T\hat w（等式两边都是标量）  \qquad \qquad$  </p><p>所以： $\left(\begin{matrix} y_1-\hat w^T\hat x_1\\y_2-\hat w^T\hat x_2 \\ \vdots \\y_m-\hat w^T\hat x_m\end{matrix}\right)=\left(\begin{matrix}y_1\\y_2 \\ \vdots \\y_m\end{matrix}\right) - \left(\begin{matrix}\hat x_1^T\hat w\\\hat x_2^T\hat w\\ \vdots \\\hat x_m^T\hat w\end{matrix}\right)=\left(\begin{matrix}y_1\\y_2 \\ \vdots \\y_m\end{matrix}\right)-\left(\begin{matrix}\hat x_1^T\\\hat x_2^T\\ \vdots \\\hat x_m^T\end{matrix}\right)\cdot\hat w=Y-X\cdot\hat w$$</p><p>所以   $E_{\hat w} = \left( y_1-\hat w^T\hat x_1 \quad  y_2-\hat w^T\hat x_2  \quad  \cdots \quad   y_m-\hat w^T\hat x_m\right)     \left(\begin{matrix} y_1-\hat w^T\hat x_1\\y_2-\hat w^T\hat x_2 \\ \vdots \\y_m-\hat w^T\hat x_m\end{matrix}\right)= \left(Y-X\hat w\right)^T\left(Y-X\hat w\right)\qquad\qquad(1.7)$</p><h3 id="1-2-4判断损失函数E的凹凸性"><a href="#1-2-4判断损失函数E的凹凸性" class="headerlink" title="1.2.4判断损失函数E的凹凸性"></a>1.2.4判断损失函数E的凹凸性</h3><p>如果$E_{\hat w} $的Hessian矩阵是正定的，则$E_{\hat w} $即是凸函数。</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial E_{\hat w} }{\partial \hat w}&=\frac{\partial}{\partial \hat w}\left[\left(Y-X\hat w\right)^T\left(Y-X\hat w\right)\right]\\&=\frac{\partial}{\partial \hat w}\left[\left(Y^T-\hat w^T X^T\right)^T\left(Y-X\hat w\right)\right]\\&=\frac{\partial}{\partial \hat w}\left[\left(Y^T-\hat w^T X^T\right)^T\left(Y-X\hat w\right)\right]\\&=\frac{\partial}{\partial \hat w}\left[Y^TY-Y^TX\hat w-\hat w^TX^TY+\hat w^TX^TX\hat w\right]\\&=\frac{\partial Y^TX\hat w}{\partial \hat w}-\frac{\partial \hat w^TX^TY}{\partial \hat w}+\frac{\partial \hat w^TX^TX\hat w}{\partial \hat w}\end{aligned}</script><p>因为： $\frac{\partial x^Ta }{\partial x}=\frac{\partial a^Tx }{\partial x}=a\qquad\frac{\partial x^Tbx }{\partial x}=(b+b^T)x\qquad\qquad$</p><p>所以：$\frac{\partial E_{\hat w} }{\partial \hat w}=-X^TY-X^TY+(X^TX+X^TX)\hat w=2X^T(X\hat w-Y)\qquad\qquad(1.8)$</p><p>接下来求2阶偏导数：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial^2 E_{\hat w} }{\partial \hat w\partial \hat w^T}&=\frac{\partial}{\partial \hat w}\left(\frac{\partial E_{\hat w} }{\partial \hat w}\right)\\&=\frac{\partial}{\partial \hat w}\left[2X^T(X\hat w-Y)\right]\\&=\frac{\partial}{\partial \hat w}\left(2X^TX\hat w-2X^TY\right)\\&=2X^TX\qquad\qquad (Hessian矩阵)\end{aligned}</script><h3 id="1-2-5求解损失函数最小时的参数的解"><a href="#1-2-5求解损失函数最小时的参数的解" class="headerlink" title="1.2.5求解损失函数最小时的参数的解"></a>1.2.5求解损失函数最小时的参数的解</h3><p>令（1.8）  $\frac{\partial E_{\hat w} }{\partial \hat w}=2X^T(X\hat w-Y)=0  \qquad得出\qquad2X^TX \hat w=2X^TY$</p><p>等式两边都乘以$(X^TX)^{-1}\qquad  得 \hat w = (X^TX)^{-1}X^TY\qquad\qquad(1.9)$</p><p>至此我们就求出了损失函数取最小值时$\hat w$的值</p><h2 id="1-3结束"><a href="#1-3结束" class="headerlink" title="1.3结束"></a>1.3结束</h2><p>前面1.1和1.2两个章节分别对一元线性回归和多元线性回归的模型的公式进行了推导，下一篇将对逻辑回归和感知机模型进行推导。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降魔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

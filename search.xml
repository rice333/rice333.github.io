<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>二零一七年终总结</title>
      <link href="/2018/01/22/2017-conclusion/"/>
      <url>/2018/01/22/2017-conclusion/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=407679465&auto=1&height=66"></iframe></div><p>时间过得很快，终于等到放寒假了，虽然这几个月没有课，天天和放假也没啥区别呢。细数一下，还有5个月就要毕业了吧，大一刚入学的场景却依然清楚地记得，转眼间就成了老学长了呢。闲来无事，随便写写，有感而发，无病呻吟而已。</p><h1 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h1><hr><p>回顾我的2017，没做什么事，令我能记得就3件大事吧：</p><ul><li>失恋ing</li><li>ACM退役</li><li>顺利保研</li></ul><p><strong>第一件事</strong>就不想过多回忆了，<strong>2014.12.13 ~ 2017.03.01</strong>，曲终人散。<br><img src="749826.jpg" alt><br>最后引用《我的少女时代》里的一句话吧。</p><blockquote><p>每人都有一颗林真心，遇见是最美好的小幸运，谢谢你出现在我的青春里。</p></blockquote><p><img src="1.jpg" alt><br><strong>第二件事</strong>其实也是黯淡退出吧，大三下开始课程繁忙，也就没怎么训练了，再加上暑假考驾照，于是乎就退役了。回顾三年来，从大一入学时电脑都没怎么碰过的小白，到现在算法也略有所知，也是付出过很大的努力吧，毕竟当年每天刷题，为了一个bug而熬夜到凌晨。最后也算是混了个水水的金牌，奖项不算耀眼。但最重要的是从这段经历中，学到了拼搏、坚持的一种精神，这对以后的研究生涯想必也有很大帮助。<br><img src="2.jpg" alt><br><strong>第三件事</strong>也是意料之中吧，没有什么波折。纠结了很多，虽然<strong>专业第一</strong>，但是最后还是选择保了本校。要问原因，也许是校园情怀，也许是导师人很好，也许是为了方便更早研究，也许就是懒吧。现在尘埃落定，靠人靠天不如靠自己，继续努力吧。<br><img src="3.jpg" alt></p><p>过去的一年，学业未有很大长进，看着同学们整页的4.0绩点，心里倒也没有什么不平衡了。下学期保了研之后选了一门研究生的文本挖掘课，也马马虎虎读了几十篇论文，也算是对自然语言处理和深度学习入了个门，最后的presentation做的还算满意。</p><h1 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h1><hr><p>今年最重要的大事莫过于毕业论文了，因为以后要做的方向是句法分析，所以导师给我的毕业论文安排的就是《基于循环神经网络的成分句法分析》。虽然说是基于ACL2013的一篇论文改编的，但是目前为止，我还没有发现有人做和这个完全一样的。也许最后写的好的话可以直接发paper了。</p><p>但是目前基本的框架还没完全搭建起来吧，代码还不是很熟练，现在只写了一个最基础的动态规划+RNN。最近有如下计划：</p><ul><li>准备试一下动态规划+LSTM。</li><li>然后动态规划扩增一个维度，用来保存左右结点的head结点。</li><li>如果这个写好了，就可以和我github找到的PCFG+CYK代码融合了，准备加上每个结点的POS。</li><li>最后加入预训练词向量应该就基本完成了。</li></ul><p>希望能顺利毕业吧，前一段时间一直对一些实现细节有些困惑，代码还写错了，还以为理论错了。也不知道最后出来的结果会怎么样，希望能不错。</p><p>生活方面，最近半年越来越懒了，极少出门，睡得晚，起的也晚。最近买了把尤克里里，也算是陶冶陶冶情操吧，不至于一直盯着电脑。现在也小有长进，能弹一点点了。</p><p>现在能聊天的人越来越少了，QQ微信放那一天也不一定会有人来找，就算有人也多半是咨询问题的，等一个可以交心的人吧。有时我也想过，我是不是太像中央空调了，对所有人都这么有耐心，到头来却还是一个人，付出那么多最后还是一无所有。<br><img src="4.jpg" alt><br>最后还是祝自己2018年顺利吧，希望毕业顺利，研究生涯小有收获，最后等一个有缘人吧。</p><blockquote><p>我遇见谁，会有怎样的对白。<br>我等的人，她在多远的未来。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型公式推导</title>
      <link href="/2018/01/22/xian-xing-hui-gui-mo-xing-xi-gua/"/>
      <url>/2018/01/22/xian-xing-hui-gui-mo-xing-xi-gua/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归公式推导"><a href="#线性回归公式推导" class="headerlink" title="线性回归公式推导"></a>线性回归公式推导</h1><h2 id="一元线性回归公式推导"><a href="#一元线性回归公式推导" class="headerlink" title="一元线性回归公式推导"></a>一元线性回归公式推导</h2><p>一元线性回归公式：$f(x_i)=wx_i+b$</p><p>损失函数为平方损失（均方误差）：$E(w,b)=\sum \limits_{i=1}^m(y_i-f(x_i))^2    \qquad\qquad\qquad m是样本个数 $</p><h3 id="推导思路-：-核心思想是最小二乘法"><a href="#推导思路-：-核心思想是最小二乘法" class="headerlink" title="推导思路 ： 核心思想是最小二乘法"></a>推导思路 ： 核心思想是最小二乘法</h3><h4 id="由最小二乘法导出平方损失-E-w-b"><a href="#由最小二乘法导出平方损失-E-w-b" class="headerlink" title="由最小二乘法导出平方损失 E(w,b)"></a>由最小二乘法导出平方损失 E(w,b)</h4><script type="math/tex; mode=display">\begin{eqnarray} E(w,b)        &=& \sum _{i=1}^m(y_i-f(x_i))^2\\        &=&\sum_{i=1}^m(y_i-(wx_i+b))^2\\        &=&\sum_{i=1}^m(y_i-wx_i-b)^2 \qquad\qquad\qquad(1.1)    \end{eqnarray}</script><h4 id="我们的目的是求出损失函数E-w-b-取最小值时，w与b的解"><a href="#我们的目的是求出损失函数E-w-b-取最小值时，w与b的解" class="headerlink" title="我们的目的是求出损失函数E(w,b)取最小值时，w与b的解"></a>我们的目的是求出损失函数E(w,b)取最小值时，w与b的解</h4><script type="math/tex; mode=display">(w^*,b^*)= \underset{(w,b)}{\operatorname{argmin}}\sum_{i=1}^m(y_i-wx_i-b)^2</script><h3 id="判断损失函数E-w-b-的凹凸性"><a href="#判断损失函数E-w-b-的凹凸性" class="headerlink" title="判断损失函数E(w,b)的凹凸性"></a>判断损失函数E(w,b)的凹凸性</h3><p>由凹凸函数的判定定理：设E(w,b)在区域D上具有二阶连续偏导数</p><p>记$A=E’’_{ww}(w,b),  B=E’’_{wb}(w,b), C=E’’_{bb}(w,b)$ 则：$A&gt;0, 且 AC-B^2\geq0$ 时，E(w,b)在区域D上是凸函数；</p><h4 id="根据上述定理我们先来求A，A为E-w-b-对w的二阶偏导数-E’’-ww-w-b"><a href="#根据上述定理我们先来求A，A为E-w-b-对w的二阶偏导数-E’’-ww-w-b" class="headerlink" title="根据上述定理我们先来求A，A为E(w,b)对w的二阶偏导数$E’’_{ww}(w,b)$"></a>根据上述定理我们先来求A，A为E(w,b)对w的二阶偏导数$E’’_{ww}(w,b)$</h4><ol><li>我们先求E(w,b)对w的一阶偏导数：$E’_w(w,b)=\frac{\partial E(w,b)}{\partial w}$</li></ol><script type="math/tex; mode=display">\begin{eqnarray} \frac{\partial E(w,b)}{\partial w}    &=&\frac{\partial }{\partial w}\bigg[\sum_{i=1}^m(y_i-wx_i-b)^2\bigg]\\    &=&\sum_{i=1}^m\frac{\partial }{\partial w}(y_i-wx_i-b)^2\\    &=&\sum_{i=1}^m2\cdot(y_i-wx_i-b)\cdot(-x_i)\\    &=&2\bigg(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)\qquad\qquad\qquad(1.2)\end{eqnarray}</script><ol><li>然后根据上述结果进而求出A。 $A=E’’_{ww}(w,b)=\frac{\partial^2 E(w,b)}{\partial w^2}$</li></ol><script type="math/tex; mode=display">\begin{eqnarray} \frac{\partial^2 E(w,b)}{\partial w^2}    &=& \frac{\partial }{\partial w}\bigg(\frac{\partial E(w,b)}{\partial w}\bigg)\\    &=& \frac{\partial }{\partial w}\bigg[2\bigg(w\sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)\bigg]\\    &=&\frac{\partial }{\partial w}\bigg[2w\sum_{i=1}^m x_i^2\bigg]\\    &=&2\sum_{i=1}^mx_i^2 \qquad\qquad\qquad       这就是A=E''_{ww}(w,b)\end{eqnarray}</script><h4 id="接下来我们来求B，B为E-w-b-对w和b的二阶混合偏导数-E’’-wb-w-b"><a href="#接下来我们来求B，B为E-w-b-对w和b的二阶混合偏导数-E’’-wb-w-b" class="headerlink" title="接下来我们来求B，B为E(w,b)对w和b的二阶混合偏导数: $E’’_{wb}(w,b)$"></a>接下来我们来求B，B为E(w,b)对w和b的二阶混合偏导数: $E’’_{wb}(w,b)$</h4><script type="math/tex; mode=display">\begin{eqnarray} E''_{wb}(w,b)    &=&\frac{\partial^2 E(w,b)}{\partial w \partial b}\\    &=&\frac{\partial}{\partial b}\bigg(\frac{\partial E(w,b)}{\partial w}\bigg)\\    &=&\frac{\partial}{\partial b}\bigg[2\bigg(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)\bigg]\\    &=&\frac{\partial}{\partial b}\bigg[-2\sum_{i=1}^m(y_i-b)x_i\bigg]\\    &=&\frac{\partial}{\partial b}\bigg(-2\sum_{i=1}^my_ix_i+2\sum_{i=1}^mbx_i\bigg)\\    &=&2\sum_{i=1}^mx_i \qquad\qquad\qquad       这就是B=E''_{wb}(w,b)\end{eqnarray}</script><h4 id="最后我们来求C，C为E-w-b-对b的二阶偏导数-E’’-bb-w-b"><a href="#最后我们来求C，C为E-w-b-对b的二阶偏导数-E’’-bb-w-b" class="headerlink" title="最后我们来求C，C为E(w,b)对b的二阶偏导数$E’’_{bb}(w,b)$"></a>最后我们来求C，C为E(w,b)对b的二阶偏导数$E’’_{bb}(w,b)$</h4><ol><li>我们先求E(w,b)对b的一阶偏导数：$E’_b(w,b)=\frac{\partial E(w,b)}{\partial b}$</li></ol><script type="math/tex; mode=display">\begin{eqnarray} \frac{\partial E(w,b)}{\partial b}    &=&\frac{\partial}{\partial b}\bigg[ \sum_{i=1}^m(y_i-wx_i-b)^2\bigg]\\    &=&\sum_{i=1}^m\frac{\partial}{\partial b}(y_i-wx_i-b)^2\\    &=&\sum_{i=1}^m2\cdot(y_i-wx_i-b)\cdot(-1)\\    &=&2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)\qquad\qquad\qquad(1.3)\end{eqnarray}</script><ol><li>然后根据上述结果进而求出C。 $C=E’’_{bb}(w,b)=\frac{\partial^2 E(w,b)}{\partial b^2}$</li></ol><script type="math/tex; mode=display">\begin{eqnarray} \frac{\partial^2 E(w,b)}{\partial b^2}    &=&\frac{\partial}{\partial b}\bigg(\frac{\partial E(w,b)}{\partial b}\bigg)\\    &=&\frac{\partial}{\partial b}\bigg[ 2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)\bigg]\\    &=&2m\qquad\qquad\qquad       这就是C=E''_{bb}(w,b)\end{eqnarray}</script><h4 id="下面我们根据上面算出的ABC的值，来证明E-w-b-的凹凸性"><a href="#下面我们根据上面算出的ABC的值，来证明E-w-b-的凹凸性" class="headerlink" title="下面我们根据上面算出的ABC的值，来证明E(w,b)的凹凸性"></a>下面我们根据上面算出的ABC的值，来证明E(w,b)的凹凸性</h4><p>我把定理贴过来再看一眼：设E(w,b)在区域D上具有二阶连续偏导数</p><p>记$A=E’’_{ww}(w,b),  B=E’’_{wb}(w,b), C=E’’_{bb}(w,b)$ 则：$A&gt;0, 且 AC-B^2\geq0$ 时，E(w,b)在区域D上是凸函数；</p><p>由上文可得：$A=2\sum\limits_{i=1}^mx_i^2\qquad\qquad B=2\sum\limits_{i=1}^mx_i\qquad\qquad C=2m$</p><p>A显然是大于0的。</p><script type="math/tex; mode=display">\begin{eqnarray} AC-B^2    &=&2m\cdot2\sum\limits_{i=1}^mx_i^2-\bigg( 2\sum\limits_{i=1}^mx_i\bigg)^2\\    &=&4m\sum\limits_{i=1}^mx_i^2-4m\cdot\frac1m\cdot\bigg( \sum\limits_{i=1}^mx_i\bigg)^2\\    &=&4m\sum\limits_{i=1}^mx_i^2-4m\cdot\bar x\cdot \sum\limits_{i=1}^mx_i\\    &=&4m\bigg( \sum\limits_{i=1}^mx_i^2-\sum\limits_{i=1}^mx_i\bar x\bigg)\\    &=&4m\sum\limits_{i=1}^m(x_i^2-x_i\bar x) \\    &=&4m\sum\limits_{i=1}^m(x_i^2-x_i\bar x-x_i\bar x+x_i\bar x)\qquad\longleftarrow\qquad因为\sum\limits_{i=1}^mx_i\bar x=m\bar x\bar x=\sum\limits_{i=1}^m\bar x^2\\    &=&4m\sum\limits_{i=1}^m(x_i-\bar x)^2 \geq0\end{eqnarray}</script><p>由上可以证明：$\color{red}{E(w,b)是关于w和b的凸函数.}$</p><h3 id="算出损失函数取最小值时w与b的值"><a href="#算出损失函数取最小值时w与b的值" class="headerlink" title="算出损失函数取最小值时w与b的值"></a>算出损失函数取最小值时w与b的值</h3><p>根据定理：设E(w,b)是在D内有连续偏导数的凸函数，$(w^<em>，b^</em>)\in D$ 且$E’_w(w^<em>,b^</em>)=0,E’_b(w^<em>,b^</em>)=0$ 则$E(w^<em>,b^</em>)$必为E(w,b)在D内的最小值。</p><p>前面的小节我们证明了损失函数E(w,b)是凸函数，所以E(w,b)取最小值时，w与b的解$w^<em>$与$b^</em>$ 满足<br>E(w,b)的一阶偏导数为0。</p><h4 id="先来求b"><a href="#先来求b" class="headerlink" title="先来求b"></a>先来求b</h4><p>令 $\frac{\partial E(w,b)}{\partial b}=0$  (即：式1.3=0)</p><script type="math/tex; mode=display">\frac{\partial E(w,b)}{\partial b}=2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)=0</script><script type="math/tex; mode=display">mb = \sum_{i=1}^m(y_i-wx_i)</script><script type="math/tex; mode=display">\begin{eqnarray}b^*    &=&\frac1m\sum_{i=1}^m(y_i-wx_i)\\    &=&\frac1m\sum_{i=1}^my_i-w\cdot\frac1m\sum_{i=1}^mx_i\\    &=&\bar y-w\bar x\\\end{eqnarray}</script><h4 id="再来求w"><a href="#再来求w" class="headerlink" title="再来求w"></a>再来求w</h4><p>$令\frac{\partial E(w,b)}{\partial w}=0$  (即：式1.2=0)</p><script type="math/tex; mode=display">\frac{\partial E(w,b)}{\partial w}=2\bigg(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)=0</script><script type="math/tex; mode=display">w\sum_{i=1}^mx_i^2=\sum_{i=1}^my_ix_i-\sum_{i=1}^mbx_i</script><script type="math/tex; mode=display">w\sum_{i=1}^mx_i^2=\sum_{i=1}^my_ix_i-\sum_{i=1}^m(\bar y-w\bar x)x_i</script><script type="math/tex; mode=display">w\sum_{i=1}^mx_i^2=\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i+w\bar x\sum_{i=1}^mx_i</script><script type="math/tex; mode=display">w\bigg(\sum_{i=1}^mx_i^2-\bar x\sum_{i=1}^mx_i\bigg)=\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i</script><script type="math/tex; mode=display">w^*=\frac{\sum\limits_{i=1}^my_ix_i-\bar y\sum\limits_{i=1}^mx_i}{\sum\limits_{i=1}^mx_i^2-\bar x\sum\limits_{i=1}^mx_i}\qquad\qquad(1.4)</script><p>因为：</p><script type="math/tex; mode=display">\bar y\sum\limits_{i=1}^mx_i=\frac1m\sum\limits_{i=1}^my_i\sum\limits_{i=1}^mx_i=\bar x\sum\limits_{i=1}^my_i\qquad\qquad(1.5)</script><script type="math/tex; mode=display">\bar x\sum\limits_{i=1}^mx_i=\frac1m\sum\limits_{i=1}^mx_i\sum\limits_{i=1}^mx_i=\frac1m(\sum\limits_{i=1}^mx_i)^2\qquad\qquad(1.6)</script><p>将1.5和1.6代入1.4可得：</p><script type="math/tex; mode=display">w^*=\frac{\sum\limits_{i=1}^my_ix_i-\bar x\sum\limits_{i=1}^my_i}{\sum\limits_{i=1}^mx_i^2-\frac1m(\sum\limits_{i=1}^mx_i)^2}=\frac{\sum\limits_{i=1}^my_i(x_i-\bar x)}{\sum\limits_{i=1}^mx_i^2-\frac1m(\sum\limits_{i=1}^mx_i)^2}</script><p>到此为止我们就求出了损失函数取最小值时的w和b的值（$w^<em>,b^</em>$）.</p><h2 id="多元线性回归公式推导"><a href="#多元线性回归公式推导" class="headerlink" title="多元线性回归公式推导"></a>多元线性回归公式推导</h2><p>多元线性回归公式：（大家一定要记住：多元线性回归里变量为向量）</p><script type="math/tex; mode=display">\begin{eqnarray}f(x_i)&=&w^Tx_i+b\\&=&(w_1 \   w_2\  \cdots \   w_d)  \left(\begin{matrix}x_{i1}\\ x_{i2}  \\ \vdots \\ x_{id}\end{matrix}\right) +b\\&=&w_1x_{i1}+w_2x_{i2}+\dots+w_dx_{id}+b\end{eqnarray}</script><p>现在我们令：</p><p>$b=w_{d+1}  $</p><script type="math/tex; mode=display">\begin{eqnarray}f(x_i)&=&(w_1 \   w_2\  \cdots \   w_d \   )  \left(\begin{matrix}x_{i1}\\ x_{i2}  \\ \vdots \\ x_{id}\end{matrix}\right) +b\\&=&(w_1 \   w_2\  \cdots \   w_d \    w_{d+1})  \left(\begin{matrix}x_{i1}\\ x_{i2}  \\ \vdots \\ x_{id} \\ 1\end{matrix}\right)=\hat w \hat x_i\end{eqnarray}</script><p>通过上面的方法，我们把w和b组合成了$\hat w$ 则：</p><script type="math/tex; mode=display">f(\hat x_i)=\hat w^T\hat x_i</script><h3 id="推导思路-：-核心思想依然是最小二乘法"><a href="#推导思路-：-核心思想依然是最小二乘法" class="headerlink" title="推导思路 ： 核心思想依然是最小二乘法"></a>推导思路 ： 核心思想依然是最小二乘法</h3><h4 id="由最小二乘法导出损失函数"><a href="#由最小二乘法导出损失函数" class="headerlink" title="由最小二乘法导出损失函数"></a>由最小二乘法导出损失函数</h4><p>损失函数(均方误差)为：$E_\hat w = \sum\limits_{i=1}^m(y_i-f(\hat x_i))^2=\sum\limits_{i=1}^m(y_i-\hat w^T \hat x_i)^2$</p><p>$X=\left(\begin{matrix}x_{11} &amp; x_{12} &amp;\cdots&amp;x_{1d}&amp;1\\ x_{12} &amp;x_{22} &amp;  \cdots&amp;  x_{2d}&amp;1 \\ \vdots &amp; \vdots  &amp;&amp;\vdots&amp;\vdots\\ x_{m1}&amp;x_{m2}&amp;\cdots&amp;x_{md}&amp;1\end{matrix}\right)= \left(\begin{matrix}x_1^T&amp;1\\ x_2^T&amp;1 \\ \vdots &amp;\vdots\\ x_m^T&amp;1\end{matrix}\right)=\left(\begin{matrix}\hat x_1^T\\ \hat x_2^T \\ \vdots \\ \hat x_m^T\end{matrix}\right)$</p><p>$Y=\left(\begin{matrix}y_1\\y_2 \\ \vdots \\y_m\end{matrix}\right)$</p><script type="math/tex; mode=display">\begin{eqnarray}E_\hat w &=&\sum\limits_{i=1}^m\left(y_i-\hat w^T \hat x_i\right)^2\\&=&\left( y_1-\hat w^T\hat x_1\right)^2+\left( y_2-\hat w^T\hat x_2\right)^2+\cdots+\left( y_m-\hat w^T\hat x_m\right)^2\\&=&\left( y_1-\hat w^T\hat x_1 \quad  y_2-\hat w^T\hat x_2  \quad  \cdots \quad   y_m-\hat w^T\hat x_m\right)     \left(\begin{matrix} y_1-\hat w^T\hat x_1\\y_2-\hat w^T\hat x_2 \\ \vdots \\y_m-\hat w^T\hat x_m\end{matrix}\right)\\\end{eqnarray}</script><p>因为： $\hat w^T\hat x_i = \hat x_i^T\hat w（等式两边都是标量）  \qquad \qquad$   所以： $\left(\begin{matrix} y_1-\hat w^T\hat x_1\\y_2-\hat w^T\hat x_2 \\ \vdots \\y_m-\hat w^T\hat x_m\end{matrix}\right)=\left(\begin{matrix}y_1\\y_2 \\ \vdots \\y_m\end{matrix}\right) - \left(\begin{matrix}\hat x_1^T\hat w\\\hat x_2^T\hat w\\ \vdots \\\hat x_m^T\hat w\end{matrix}\right)=\left(\begin{matrix}y_1\\y_2 \\ \vdots \\y_m\end{matrix}\right)-\left(\begin{matrix}\hat x_1^T\\\hat x_2^T\\ \vdots \\\hat x_m^T\end{matrix}\right)\cdot\hat w=Y-X\cdot\hat w$</p><p>所以   $ E_\hat w  = \left( y_1-\hat w^T\hat x_1 \quad  y_2-\hat w^T\hat x_2  \quad  \cdots \quad   y_m-\hat w^T\hat x_m\right)     \left(\begin{matrix} y_1-\hat w^T\hat x_1\\y_2-\hat w^T\hat x_2 \\ \vdots \\y_m-\hat w^T\hat x_m\end{matrix}\right)= \left(Y-X\hat w\right)^T\left(Y-X\hat w\right)\qquad\qquad(1.7)$</p><h3 id="判断损失函数E的凹凸性"><a href="#判断损失函数E的凹凸性" class="headerlink" title="判断损失函数E的凹凸性"></a>判断损失函数E的凹凸性</h3><p>如果$E_\hat w $的Hessian矩阵是正定的，则$E_\hat w $即是凸函数。</p><script type="math/tex; mode=display">\begin{eqnarray}\frac{\partial E_\hat w }{\partial \hat w}&=&\frac{\partial}{\partial \hat w}\left[\left(Y-X\hat w\right)^T\left(Y-X\hat w\right)\right]\\&=&\frac{\partial}{\partial \hat w}\left[\left(Y^T-\hat w^T X^T\right)^T\left(Y-X\hat w\right)\right]\\&=&\frac{\partial}{\partial \hat w}\left[\left(Y^T-\hat w^T X^T\right)^T\left(Y-X\hat w\right)\right]\\&=&\frac{\partial}{\partial \hat w}\left[Y^TY-Y^TX\hat w-\hat w^TX^TY+\hat w^TX^TX\hat w\right]\\&=&\frac{\partial Y^TX\hat w}{\partial \hat w}-\frac{\partial \hat w^TX^TY}{\partial \hat w}+\frac{\partial \hat w^TX^TX\hat w}{\partial \hat w}\end{eqnarray}</script><p>因为： $\frac{\partial x^Ta }{\partial x}=\frac{\partial a^Tx }{\partial x}=a\qquad\frac{\partial x^Tbx }{\partial x}=(b+b^T)x\qquad\qquad所以：\frac{\partial E_\hat w }{\partial \hat w}=-X^TY-X^TY+(X^TX+X^TX)\hat w=2X^T(X\hat w-Y)\qquad\qquad(1.8)$</p><p>接下来求2阶偏导数：</p><script type="math/tex; mode=display">\begin{eqnarray}\frac{\partial^2 E_\hat w }{\partial \hat w\partial \hat w^T}&=&\frac{\partial}{\partial \hat w}\left(\frac{\partial E_\hat w }{\partial \hat w}\right)\\&=&\frac{\partial}{\partial \hat w}\left[2X^T(X\hat w-Y)\right]\\&=&\frac{\partial}{\partial \hat w}\left(2X^TX\hat w-2X^TY\right)\\&=&2X^TX\qquad\qquad (Hessian矩阵)\end{eqnarray}</script><h3 id="求解损失函数最小时的参数的解"><a href="#求解损失函数最小时的参数的解" class="headerlink" title="求解损失函数最小时的参数的解"></a>求解损失函数最小时的参数的解</h3><p>令（1.8）  $\frac{\partial E_\hat w }{\partial \hat w}=2X^T(X\hat w-Y)=0  \qquad得出\qquad2X^TX \hat w=2X^TY$</p><p>等式两边都乘以$(X^TX)^{-1}\qquad  得 \hat w = (X^TX)^{-1}X^TY\qquad\qquad(1.9)$</p><h2 id="逻辑回归公式推导"><a href="#逻辑回归公式推导" class="headerlink" title="逻辑回归公式推导"></a>逻辑回归公式推导</h2><h3 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h3><p>分布律满足下式即为指数族分布：</p><script type="math/tex; mode=display">p(y;\eta) = b(y) exp(\eta ^TT(y)-a(\eta))</script><p>假设y取值为1时的概率为$\phi$</p><p>伯努力分布分布律为：</p><script type="math/tex; mode=display">\begin{eqnarray} p(y)&=&\phi ^y(1-\phi)^{1-y} \\&=&exp(ln(\phi^y(1-\phi)^{1-y}))\\&=&exp(y ln\phi + (1-y)ln(1-\phi))\\&=&exp(y(ln\phi-ln(1-\phi))+ln(1-\phi))\\&=&exp\left(yln\left(\frac{\phi}{1-\phi}\right) +ln(1-\phi)\right)\end{eqnarray}</script><p>对照指数族分布的分布律可得：    $b(y)=1 \qquad \eta = ln\left(\frac{\phi}{1-\phi}\right) \qquad T(y) = y \qquad a(\eta) = -ln(1-\phi) = ln(1+e^{\eta})$</p><p>所以伯努力分布为指数族分布。</p><h3 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h3><p>广义线性模型满足下面三条假设：</p><ol><li>在给定x的条件下，假设y服从某个指数族分布。</li><li>在给定x的条件下，我们的目标是得到一个模型h(x)能预测出T(y)的期望值。</li><li>假设该指数族分布中的参数$\eta与x呈线性关系：\eta = w^Tx$</li></ol><p>假设逻辑回归的变量y服从伯努力分布。前面证明了满足第一条假设。下面来看第二条假设：</p><script type="math/tex; mode=display">h(x)=E[T(y)]=E(y)</script><script type="math/tex; mode=display">E[y]=1\cdotp(y=1)+0\cdotp(y=0)=p(y=1)=\phi</script><script type="math/tex; mode=display">h(x)=\phi\qquad\qquad(1.10)</script><script type="math/tex; mode=display">\eta =ln\left(\frac{\phi}{1-\phi}\right)</script><script type="math/tex; mode=display">e^{\eta}=\frac{\phi}{1-\phi}</script><script type="math/tex; mode=display">e^{-\eta}=\frac{1}{\phi}-1</script><script type="math/tex; mode=display">\phi = \frac{1}{1+e^{-\eta}}</script><script type="math/tex; mode=display">h(x)= \frac{1}{1+e^{-\eta}}</script><p>根据第三条假设：<script type="math/tex">h(x)= \frac{1}{1+e^{-w^Tx}}</script></p><h3 id="逻辑回归的极大似然估计"><a href="#逻辑回归的极大似然估计" class="headerlink" title="逻辑回归的极大似然估计"></a>逻辑回归的极大似然估计</h3><p>$y_1,y_2,\cdots,y_m是独立同分布的，所以他们的联合概率密度函数为L(w)=\prod\limits_{i=1}^mp(y_i|x_i;w,b)\qquad此为似然函数$</p><p>lnL(w)与L(w)有相同最大值点 $lnL(w)=\sum\limits_{i=1}^mlnp(y_i|x_i;w,b)$</p><p>$p(y=1|x) = \frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\qquad$<br>$p(y=0|x) = \frac1{1+e^{w^Tx+b}}$</p><p>令$w^Tx+b = \hat w \hat x$</p><p>$p(y=1|x) = \frac{e^{\hat w \hat x}}{1+e^{\hat w \hat x}}=p_1(\hat x;\hat w)\qquad$<br>$p(y=0|x) = \frac1{1+e^{\hat w \hat x}}=p_0(\hat x;\hat w)$</p><p>把上面的两个式子写在一起为：</p><script type="math/tex; mode=display">p(y|x;w,b)=[p_1(\hat x;\hat w)]^y[ p_0(\hat x;\hat w)]^{1-y}\qquad\qquad(1.10)</script><p>对数似然函数为：</p><script type="math/tex; mode=display">\begin{eqnarray} lnL(w,b)&=&\sum\limits_{i=1}^mlnp(y_i|x_i;w,b)\\&=&\sum\limits_{i=1}^mln[p_1(\hat x_i;\hat w)]^y[ p_0(\hat x_i;\hat w)]^{1-y}\\&=&\sum\limits_{i=1}^m[y_iln(p_1(\hat x_i;\hat w))+(1-y_i)ln(p_0(\hat x_i;\hat w))]\\&=&\sum\limits_{i=1}^m[y_iln\left(\frac{p_1(\hat x_i;\hat w)}{p_0(\hat x_i;\hat w)}\right)+ln(p_0(\hat x_i;\hat w))]\end{eqnarray}</script><p>代入：$p(y=1|x) = \frac{e^{\hat w \hat x}}{1+e^{\hat w \hat x}}=p_1(\hat x;\hat w)\qquad$<br>$p(y=0|x) = \frac1{1+e^{\hat w \hat x}}=p_0(\hat x;\hat w)$</p><script type="math/tex; mode=display">\begin{eqnarray} lnL(w,b)&=&\sum\limits_{i=1}^m\left[y_iln(e^{\hat w^T\hat x_i})+ln\left(\frac{1}{1+e^{\hat w^T\hat x_i}}\right)\right]\\&=&\sum\limits_{i=1}^m\left(y_i\hat w^T\hat x_i-ln(1+e^{\hat w^T\hat x_i})\right)\end{eqnarray}</script>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
